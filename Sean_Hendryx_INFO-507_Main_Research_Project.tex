\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
%\usepackage{natbib}
\usepackage{setspace}
% ^ Getting error: natbib  Bib not compatible with author-year citations

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Transfer Learning and Ensembles for Robust Segmentation}

\author{Sean M. Hendryx\\
University of Arizona\\
Tucson, Arizona 85721\\
{\tt\small seanmhendryx@email.arizona.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

\doublespacing

%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Automated semantic image segmentation has many use cases in computer vision and
machine learning with important applications in robotics and biomedical imaging.
Accurate perception and object detection require fine resolution image segmentation.
A more robust vision system would be able to perform semantic segmentation well across
multiple domains without requiring building an entirely new architecture or retraining.


%-------------------------------------------------------------------------
\section{Related Work}

\subsection{Transfer Learning}
Transfer learning, multitask learning, and domain adaptation are tightly linked
concepts that allow deep learning systems to generalize better and be developed more rapidly.
Transfer learning broadly has been noted as "the next driver of [machine learning] success" after supervised learning by Andrew Ng in his NIPS 2016 keynote speech \cite{ng2016nips}. Learning from related tasks has a long history in machine learning beginning with \citet{thrun1996learning} and \citet{caruana1997multitask} \cite{donahue2014decaf}.
Transfer learning is the process of exploiting what has been learned in one task by applying it to another task \cite[chap. 15]{goodfellow2016deep}, thereby allowing
more rapid development of models for different tasks by applying what has been learned on one source task to a new target task. Domain adaptation is
similar to transfer learning but has been defined as differing in that the task remains the same (or very similar) while
the distribution of the source input data is different from the distribution of the target input data \cite[chap. 15]{goodfellow2016deep}. Domain adaptation can
be seen as a special case of multitask learning with one task in one domain and one task in another domain \cite{patel2015visual}. Given that these concepts overlap and that
in the wild there will be many variations and combinations of new domain and new task, I will further use the term "transfer learning" to refer to any deep learning process that utilizes
a previously learned representation.


\subsection{Semantic Segmentation}
Deep convolutional neural networks (CNNs) were first notably applied to whole image classification and the repeated convolutions, pooling layers, and final fully connected layer do not maintain spatial information necessary for segmentation \cite{marmanis2016semantic}.
Full convolutional neural networks (FCNNs) allow for semantic segmentation by learning to reverse the down-sampling with deconvolutional layers.
% move to related works:
Chen et al proposed a method \cite{chen2018deeplab} address this problem by proposing methods that improve the performance of
deep convolutional neural networks on fine semantic segmentation.

\subsection{Ensembling}
"Deep networks are notorious for hav- ing extremely non-convex, high-dimensional loss functions with many local minima.3 If one initialises with different (pre-trained, see next paragraph) sets of parameters, the net is therefore vir- tually guaranteed to converge to different solutions, even though it sees the same training data. This observation suggests a sim- ple model averaging (ensemble learning) procedure: train several networks with different initialisations, and average their predic- tions. Our results indicate that, as observed previously for image- level classification, e.g. [Simonyan and Zisserman, 2015], aver- aging multiple CNN instances further boosts performance." \cite{marmanis2016semantic}


\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\section{Data and Proposed Methods}

\section{Preliminary Results}

%------------------------------------------------------------------------
\section{Discussion and Future Work}

If such an ensemble works, it could be used in a compression architecture to train a compressed, student network.
{\small
\bibliographystyle{ieee}
\bibliography{references}
}

\end{document}
